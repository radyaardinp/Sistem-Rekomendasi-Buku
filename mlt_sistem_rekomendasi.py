# -*- coding: utf-8 -*-
"""MLT_Sistem Rekomendasi.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10iM0TAkt0ww4MCkINDT3-v46wdXx6NLM

Sistem Rekomendasi Buku

- Nama: Radya Ardi Ninang Pudyastuti
- Dataset: https://www.kaggle.com/datasets/arashnic/book-recommendation-dataset/data

----------------

# Import Library

Langkah pertama adalah melakukan import library yang dibutuhkan
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import kagglehub
import pickle
import os
import re
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.feature_extraction.text import TfidfVectorizer
import tensorflow as tf
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
import matplotlib.pyplot as plt
import numpy as np


import warnings
warnings.filterwarnings('ignore')

"""# Loading Dataset

Langkah selanjutnya adalah melakukan loading dataset yang berasal dari kaggle, kemudian dibaca ke dalam bentuk DataFrame Pandas.

Dari link tersebut, akan digunakan 2 dataset, yaitu book dan rating, untuk membangun content dan collaborative filtered book recommendation system.
"""

# Download latest version
path = kagglehub.dataset_download("arashnic/book-recommendation-dataset")
print("Path to dataset files:", path)

book = os.path.join(path, "Books.csv")
rating = os.path.join(path, "Ratings.csv")

"""menampilkan dataset book"""

df_book = pd.read_csv(book)
df_book

"""menampilkan dataset rating"""

df_rating = pd.read_csv(rating)
df_rating

"""# Eksplorasi Data Analysis (EDA)

##### Books

Pada tahap Eksplorasi Data atau Data Understanding akan dilakukan bertahap sesuai dengan dataset masing-masing. yang pertama adalah dataset books.
"""

# memeriksa ukuran dimensi data untuk mengetahui berapa jumlah baris dan kolom pada dataset book.
print(f'dimensi dataset buku:', df_book.shape)

"""pada data buku, memiliki 271360 baris dan 8 kolom"""

# memeriksa nilai yang hilang (missing value) pada dataset buku
df_book.isnull().sum()

"""berdasarkan tabel di atas, terdapat beberapa missing value pada data buku, yaitu pada kolom Book-Author, Publisher, dan Image-URL-L. dalam hal ini akan dilakukan penanganan dengan cara imputasi data pada kolom Book-Author dan Publisher, sedangkan pada kolom Image-URL-L akan dilakukan penghapusan."""

# memeriksa jumlah nilai yang duplikat pada judul buku (Book-Title) pada dataset.
df_book.duplicated(["Book-Title"]).sum()

"""dari hasil di atas didapatkan data duplikat dari data books berjumlah 29225"""

# Memeriksa tipe data pada setiap kolom
df_book.info()

"""dari hasil pengecekan di atas, tipe data tiap kolom sudah sesuai. sehingga tidak memerlukan penanganan."""

#melihat jumlah karya yang dihasilkan oleh penulis buku (Book-Author)
df_book['Book-Author'].value_counts().head(10)

"""Dari hasil analisis, diperoleh daftar 10 besar penulis dengan jumlah buku terbanyak. Penulis dengan jumlah terbanyak adalah **Agatha Christie** dengan 632 buku, diikuti oleh **William Shakespeare** (567 buku) dan **Stephen King** (524 buku).

"""

#melihat jumlah karya yang dihasilkan oleh suatu publisher
df_book['Publisher'].value_counts().head(10)

"""Berdasarkan hasil agregasi data:
- **Harlequin** menjadi penerbit dengan jumlah buku terbanyak, yaitu sebanyak 7.535 buku.
- Diikuti oleh **Silhouette** (4.220 buku), **Pocket** (3.905 buku), dan **Ballantine Books** (3.783 buku).

### Rating
"""

# Memeriksa dimensi ukuran dataset rating
print(f'dimensi dataset rating:', df_rating.shape)

"""dari hasil di atas, data rating memiliki 1149780 baris dan 3 kolom"""

# Memeriksa jumlah nilai yang hilang (missing value) pada dataset rating
df_rating.isnull().sum()

"""Dari hasil di atas, data rating tidak memiliki missing value. sehingga tidak perlu dilakukan penanganan"""

# Memeriksa nilai yang duplikat pada dataset rating
df_rating.duplicated().sum()

"""data rating tidak memiliki data duplikat. sehingga tidak perlu dilakukan penanganan"""

# memeriksa tipe data pada setiap fitur kolom
df_rating.info()

"""dari informasi di atas, data rating sudah memiliki tipe data yang sesuai"""

#Menghitung jumlah buku berdasarkan ratingnya.
df_rating['Book-Rating'].value_counts()

"""berdasarkan tabel di atas, didapatkan insight sebagai berikut:
- Nilai **0**  paling banyak muncul (**>716 ribu entri**), menandakan bahwa rating ini kemungkinan besar memberikan informasi bahwa user **tidak memberikan rating eksplisit**.
- Nilai **8–10** juga sangat dominan, menunjukkan adanya kecenderungan pengguna untuk memberikan rating yang **positif**, atau user menyukai buku tersebut.
- Nilai **1–3** sangat jarang diberikan, yang mungkin menunjukkan bias positif pada sistem.

"""

#memeriksa pengguna yang memberikan rating terbanyak dalam dataset
df_rating['User-ID'].value_counts().head(10)

"""Tabel di atas menunjukkan bahwa hanya sebagian kecil pengguna yang sangat aktif memberikan rating buku, dengan User-ID 11676 menjadi yang paling aktif sebanyak 13.602 rating. Pola ini mencerminkan ketimpangan distribusi kontribusi user (power-law), yang umum dalam sistem rekomendasi. Pengguna-pengguna aktif seperti ini bisa sangat berguna dalam pelatihan model collaborative filtering, sementara pengguna pasif mungkin menimbulkan tantangan seperti cold-start.

"""

#Menampilkan buku yang paling sering diberikan rating berdasarkan nomor ISBN.
df_rating['ISBN'].value_counts().head(10)

"""Hasil ini menunjukkan 10 buku dengan ISBN yang paling banyak mendapat rating dari pengguna, dengan ISBN `0971880107` menempati urutan pertama sebanyak 2.502 rating. Buku-buku ini kemungkinan merupakan buku populer yang dikenal luas oleh pengguna. Informasi ini penting untuk sistem rekomendasi karena item-item populer seperti ini cenderung memiliki representasi yang lebih kuat dalam model, sedangkan buku dengan sedikit rating mungkin kurang informatif dan berisiko terkena cold-start.

# Data Preprocessing
"""

books = df_book
ratings = df_rating

"""### Books

Langkah data preprocessing yang pertama pada dataset books adalah mengatasi missing value pada fitur Book-Author, Publisher, dan Image-URL-L.

pada kolom Book-Author, Publisher akan diisi dengan "Unknown". sedangkan pada kolom Image-URL-L, data yang kosong akan di drop, karena jumlahnya masih sedikit.
"""

#Mengisi missing value
books['Book-Author'].fillna('Unknown', inplace=True)
books['Publisher'].fillna('Unknown', inplace=True)
books.dropna(subset=['Image-URL-L'], inplace=True)

books.isnull().sum()

"""Langkah selanjutnya adalah menghapus data duplikat pada dataset books, agar tidak ada data yang redundan."""

books.duplicated().sum()
books.drop_duplicates(inplace=True)

books.duplicated().sum()

"""Mengubah kolom Year-Of-Publication menjadi tipe data numerik (int) dan membersihkan data tahun yang tidak valid."""

# mengubah Year-Of-Publication ke int
books['Year-Of-Publication'] = pd.to_numeric(books['Year-Of-Publication'], errors='coerce')
books = books[(books['Year-Of-Publication'] >= 1000) & (books['Year-Of-Publication'] <= 2025)]

"""Melakukan normalisasi teks dengan mengubah huruf menjadi lowercase dan menghapus spasi berlebih di awal/akhir string agar konsisten dalam format penulisan."""

# lowercase dan strip whitespace
books['Book-Title'] = books['Book-Title'].str.lower().str.strip()
books['Book-Author'] = books['Book-Author'].str.lower().str.strip()
books['Publisher'] = books['Publisher'].str.lower().str.strip()

books

"""dataframe di atas menampilkan hasil normalisasi pada kolom Book-title, book-author, dan publisher dimana seluruh huruf sudah dalam bentuk huruf kecil (tidak ada huruf kapital).

Menyaring buku-buku populer yang telah mendapatkan lebih dari 50 rating.
"""

# Filter buku populer
popular_books = ratings['ISBN'].value_counts()
popular_isbn = popular_books[popular_books > 50].index
books_filtered = books[books['ISBN'].isin(popular_isbn)].drop_duplicates(subset='Book-Title')

"""Membuat fitur gabungan dari beberapa kolom informasi buku sebagai dasar analisis yang digunakan untuk proses transformasi teks dan pencocokan kemiripan antar buku dalam content-based filtering."""

# Menggabungkan fitur yang relevan
books_filtered['combined_features'] = (
    books_filtered['Book-Title'].fillna('') + ' ' +
    books_filtered['Book-Author'].fillna('') + ' ' +
    books_filtered['Publisher'].fillna('')
)

books_filtered

"""berdasarkan dataframe di atas, telah dilakukan filtering terhadap buku-buku populer yang telah memiliki lebih dari 50 rating, dibuktikan dengan berkurangnya jumlah baris yang ada pada dataframe ini, yaitu 1889 baris. selain itu, pada data ini sudah bertambah kolom combined feature yang memuat kolom book-title, book-author, dan publisher.

Mengubah teks gabungan menjadi representasi numerik menggunakan metode TF-IDF yang digunakan untuk proses pencocokan kemiripan antar buku dalam content-based filtering. Proses ini menghasilkan matriks fitur dari kata-kata penting yang merepresentasikan karakteristik tiap buku.
"""

# TF-IDF Vectorization
tfidf = TfidfVectorizer(stop_words='english', max_features=5000)

# Fit and transform the combined features
tfidf_matrix = tfidf.fit_transform(books_filtered['combined_features'])

# Now you can get the feature names (optional, but good for inspection)
feature_names = tfidf.get_feature_names_out()
feature_names

"""memeriksa dimensi matriks TF-IDF"""

# Fit and transform the combined features
tfidf_matrix = tfidf.fit_transform(books_filtered['combined_features'])
tfidf_matrix.shape

"""Mengonversi matriks TF-IDF dari format sparse matrix menjadi dense matrix"""

tfidf_matrix.todense()

"""Membuat DataFrame dari matriks TF-IDF dalam format dense, dengan baris berupa judul buku dan kolom berupa kata-kata fitur, lalu menampilkan sampel acak 10 kata dan 10 buku untuk eksplorasi data dan pemahaman terhadap representasi teks hasil vektorisasi."""

pd.DataFrame(
    tfidf_matrix.todense(),
    columns=tfidf.get_feature_names_out(),
    index=books_filtered['Book-Title']
).sample(10, axis=1,replace=True).sample(10, axis=0)

"""Tabel ini menunjukkan vektor representasi tiap buku berdasarkan hasil TF-IDF. Baris adalah judul buku, dan kolom adalah kata-kata unik yang diekstraksi. Nilai 0.0 berarti kata tidak relevan untuk buku tersebut. Matriks ini digunakan untuk menghitung kemiripan antar buku dalam Content-Based Filtering.

Menghitung skor kemiripan antar buku berdasarkan cosine similarity dari matriks TF-IDF, yang digunakan untuk menentukan seberapa mirip satu buku dengan buku lainnya dalam content-based filtering. Hasilnya berupa matriks simetri dengan nilai antara 0 hingga 1.
"""

# Menghitung cosine similarity antar buku
cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)
cosine_sim

"""Membuat indeks pencarian berdasarkan judul buku untuk mempermudah proses pencarian posisi buku tertentu dalam data. Indeks ini digunakan saat mengambil skor kemiripan dari matriks cosine similarity."""

# Membuat indeks pencarian berdasarkan judul
indices = pd.Series(range(len(books_filtered)), index=books_filtered['Book-Title']).drop_duplicates()

"""### Rating

Menghapus data rating dengan nilai 0 yang dianggap tidak merepresentasikan penilaian sebenarnya.
"""

#Menghapus rating 0
print(f"Sebelum dibersihkan: {df_rating.shape}")
ratings_cleaned = df_rating[df_rating['Book-Rating'] > 0]
print(f"Setelah dibersihkan: {ratings_cleaned.shape}")

"""Menyaring data rating agar hanya mencakup buku yang aktif (minimal 10 rating) dan pengguna yang aktif (minimal 5 rating)."""

active_books = ratings_cleaned['ISBN'].value_counts()[ratings_cleaned['ISBN'].value_counts() >= 10].index
active_users = ratings_cleaned['User-ID'].value_counts()[ratings_cleaned['User-ID'].value_counts() >= 5].index

ratings_filtered = ratings_cleaned[ratings_cleaned['ISBN'].isin(active_books) & ratings_cleaned['User-ID'].isin(active_users)]
ratings_filtered

"""Dataframe di atas menampilkan hasil filtering yang menghasilkan 107.729 interaksi dari pengguna dan buku yang aktif.

Menyalin data rating hasil filtering dan melakukan encoding terhadap User-ID dan ISBN menjadi format angka dengan LabelEncoder, agar dapat digunakan dalam model collaborative filtering yang memerlukan input numerik
"""

ratings = ratings_filtered.copy()

# Encode user dan book jadi integer index
from sklearn.preprocessing import LabelEncoder

user_enc = LabelEncoder()
book_enc = LabelEncoder()

"""Mengubah nilai asli User-ID dan ISBN menjadi representasi angka menggunakan hasil encoding, dan menyimpannya ke kolom baru (user dan book)."""

ratings['user'] = user_enc.fit_transform(ratings['User-ID'])
ratings['book'] = book_enc.fit_transform(ratings['ISBN'])

"""Menghitung jumlah unik pengguna (user) dan buku (book) dalam data rating. Informasi ini digunakan untuk mengetahui dimensi dari matriks interaksi user-item yang akan dibentuk dalam collaborative filtering."""

num_users = ratings['user'].nunique()
num_books = ratings['book'].nunique()

"""Memilih hanya kolom user, book, dan Book-Rating dari dataset untuk membentuk matriks interaksi yang merepresentasikan hubungan antara pengguna dan buku dalam collaborative filtering."""

ratings = ratings[['user', 'book', 'Book-Rating']]

"""Membagi data menjadi data latih dan data uji untuk mempersiapkan proses pelatihan dan evaluasi model dengan ratio 80:20"""

#Splitting data
X = ratings[['user', 'book']]
y = ratings['Book-Rating']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

"""# Pemodelan

Dalam proyek ini, sistem rekomendasi dibangun dengan memanfaatkan dua pendekatan utama, yaitu:
1. **Content-Based Filtering** memberikan rekomendasi berdasarkan kemiripan atribut antar item (dalam hal ini, buku). Sistem ini menganalisis fitur-fitur seperti penulis, judul, dan informasi buku lainnya, lalu mencari buku yang memiliki karakteristik serupa dengan buku yang disukai pengguna sebelumnya. Pendekatan ini tidak bergantung pada data dari pengguna lain, melainkan fokus pada konten dari item itu sendiri.
2. **Collaborative Filtering** memberikan rekomendasi berdasarkan perilaku pengguna lain. Sistem ini mengasumsikan bahwa jika dua pengguna memiliki selera yang mirip, maka buku yang disukai oleh satu pengguna kemungkinan juga disukai oleh pengguna lainnya. Pendekatan ini menggunakan interaksi antar pengguna dan item, tanpa memerlukan informasi detail dari konten buku itu sendiri.

-------------------
## Content Based Section
"""

# Fungsi rekomendasi berdasarkan judul buku
def recommend_books_content_based(title, cosine_sim=cosine_sim, top_n=5):
    idx = indices.get(title)
    if idx is None:
        return ["Judul tidak ditemukan."]

    idx = int(idx)
    if idx >= cosine_sim.shape[0]:
        return ["Judul ditemukan, tetapi indeksnya tidak valid dalam matriks kesamaan."]


    sim_scores = list(enumerate(cosine_sim[idx]))
    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)[1:top_n+1]
    book_indices = [i[0] for i in sim_scores]

    try:
        recommended_books_info = books_filtered.iloc[book_indices][['Book-Title', 'Book-Author', 'Publisher']]
    except IndexError:
        return ["Terjadi kesalahan saat mengambil informasi buku. Mungkin ada masalah dengan indeks buku yang direkomendasikan."]

    return recommended_books_info.reset_index(drop=True)

"""Fungsi ini menghasilkan rekomendasi **5 buku serupa** berdasarkan judul buku yang diberikan. Sistem menggunakan **cosine similarity** antar judul buku yang telah ditransformasikan dengan **TF-IDF**, kemudian memilih buku dengan skor kemiripan tertinggi. Informasi yang ditampilkan mencakup judul, penulis, dan penerbit buku.

"""

# Membuat ground truth dari user yang memberi rating >= 7
user_likes = ratings_filtered[ratings_filtered['Book-Rating'] >= 7]
user_true_books = user_likes.groupby('User-ID')['ISBN'].apply(list).to_dict()

# Mengambil rekomendasi untuk user berdasarkan buku yang pernah mereka baca
def get_user_recommendations(user_id, top_n=5):
    user_books = ratings_filtered[ratings_filtered['User-ID'] == user_id]['ISBN'].tolist()
    recommended_isbns = set()

    for isbn in user_books:
        title_row = books[books['ISBN'] == isbn]
        if not title_row.empty:
            title = title_row['Book-Title'].values[0]
            # Call the content-based recommendation function
            recs = recommend_books_content_based(title)
            if isinstance(recs, pd.DataFrame):
                # Need to get ISBNs for the recommended titles from the original 'books' dataframe
                # as 'recs' only contains title, author, publisher
                rec_isbns = books[books['Book-Title'].isin(recs['Book-Title'])]['ISBN'].tolist()
                recommended_isbns.update(rec_isbns)

    # Filter out books the user has already read from the recommendations
    already_read_isbns = ratings_filtered[ratings_filtered['User-ID'] == user_id]['ISBN'].tolist()
    recommended_isbns = [isbn for isbn in recommended_isbns if isbn not in already_read_isbns]

    # Let's get the titles of the books the user has already read
    already_read_titles = books[books['ISBN'].isin(already_read_isbns)]['Book-Title'].tolist()

    # Convert recommended ISBNs back to titles for the return
    recommended_titles = books[books['ISBN'].isin(list(recommended_isbns))]['Book-Title'].tolist()

    # Return both recommended titles and already read titles
    return recommended_titles[:top_n], already_read_titles[:top_n] # Limit both lists to top_n for consistency with the call

"""Fungsi `get_user_recommendations()` menghasilkan daftar rekomendasi buku untuk seorang user berdasarkan buku-buku yang telah mereka beri rating ≥ 7. Untuk setiap buku yang telah dibaca user, sistem memanggil fungsi content-based filtering untuk mencari buku serupa berdasarkan judul.

Rekomendasi dikembalikan dalam bentuk:
- **Buku yang direkomendasikan (judul)** yang belum pernah dibaca user
- **Daftar buku yang sudah pernah dibaca** (judul), sebagai referensi

Rekomendasi dibatasi sejumlah `top_n` buku (default 5 judul).
"""

# Precision@K evaluasi (update the function call within this as well)
def precision_at_k_content_based(user_true_books, top_n=5):
    precision_scores = []
    tested_users = list(user_true_books.keys())[:100]  # Sampling user

    for user_id in tested_users:
        true_books = set(user_true_books[user_id])
        # Call the content-based recommendation function and unpack only the recommended titles
        rec_books, _ = get_user_recommendations(user_id, top_n)

        # We need to compare ISBNs, not titles, for evaluation against user_true_books
        # Convert recommended titles back to ISBNs
        rec_isbns = books[books['Book-Title'].isin(rec_books)]['ISBN'].tolist()

        if not rec_isbns:
            continue

        hits = sum([1 for isbn in rec_isbns if isbn in true_books])
        precision_scores.append(hits / top_n)

    if not precision_scores: # Handle case where no users were tested or no recommendations were generated
        return 0.0

    return sum(precision_scores) / len(precision_scores)

"""Fungsi `precision_at_k_content_based()` menghitung rata-rata precision dari 100 user terpilih.  
Evaluasi dilakukan dengan membandingkan ISBN buku rekomendasi terhadap buku yang pernah diberi rating ≥ 7.  
Metrik ini menunjukkan seberapa relevan rekomendasi yang diberikan sistem dalam Top-K pilihan.

Bagian ini menjalankan evaluasi akurasi rekomendasi menggunakan metrik **Precision@5**, serta menguji fungsi rekomendasi berdasarkan data riil pengguna (contoh: user ID 276688).  
Output menunjukkan daftar buku yang telah dibaca user dan rekomendasi buku baru dari sistem content-based filtering.
"""

# Menjalankan evaluasi
precision_cb = precision_at_k_content_based(user_true_books, top_n=5)
print(f"Precision@5 untuk Content-Based Filtering: {precision_cb:.4f}")

user_id = 276688
# The get_user_recommendations function now returns two lists: recommended titles and already read titles
recommended_books, already_read = get_user_recommendations(user_id, top_n=5)

print("Buku yang sudah dibaca dan dinilai user:")
for book in already_read:
    print("-", book)

print("\nRekomendasi buku untuk user:")
for book in recommended_books:
    print("-", book)

"""Hasil evaluasi menunjukkan nilai **Precision@5 sebesar 0.0093**, yang berarti bahwa dari 5 rekomendasi yang diberikan untuk masing-masing user, rata-rata hanya sekitar 0.0093 (kurang dari 1%) yang benar-benar sesuai dengan preferensi pengguna (berdasarkan data rating ≥ 7).

Hasil rekomendasi untuk user ID 276688:
- **Buku yang telah dibaca**: *basket case*, *stalker*, *in pursuit of the proper sinner*, dll.
- **Rekomendasi sistem**: *lucky you*, *sick puppy*, *ashes to ashes*, dll.

Meskipun rekomendasi tampak relevan secara tematik, nilai presisinya masih rendah, mengindikasikan bahwa penyempurnaan fitur atau pendekatan mungkin dibutuhkan.

Pada bagian ini dilakukan pengujian sistem rekomendasi content-based filtering menggunakan input judul buku **"american gods"**. Sistem akan mencari dan menampilkan buku-buku lain yang memiliki kemiripan konten berdasarkan fitur teks judul dan deskripsi lainnya.
"""

# uji coba model with the renamed function
print("\nUji coba rekomendasi buku berdasarkan judul:")
print(recommend_books_content_based('american gods'))

"""Berdasarkan hasil output uji coba model di atas, sistem berhasil menghasilkan daftar buku yang memiliki kemiripan konten, ditunjukkan dari kemunculan karya-karya lain dari penulis yang sama (Neil Gaiman), serta buku lain dengan karakteristik serupa.

----
## Collaborative Filtering

Membangun model collaborative filtering berbasis Neural Collaborative Filtering (NCF) dengan arsitektur neural network. Model ini menggunakan embedding untuk memetakan user dan item ke dalam vektor dimensi rendah, kemudian menggabungkannya dan melewatkannya ke layer dense untuk mempelajari pola interaksi yang kompleks antar pengguna dan buku. Model ini dikompilasi menggunakan fungsi loss mse (mean squared error) dan optimizer adam.
"""

#Membangun model Neural Network Model (NCF)
from tensorflow.keras.layers import Input, Embedding, Flatten, Concatenate, Dense, Dropout
from tensorflow.keras.models import Model

# Input
user_input = Input(shape=(1,))
book_input = Input(shape=(1,))

# Embedding
user_embedding = Embedding(num_users, 32)(user_input)
book_embedding = Embedding(num_books, 32)(book_input)

user_vec = Flatten()(user_embedding)
book_vec = Flatten()(book_embedding)

# Concatenate
concat = Concatenate()([user_vec, book_vec])
dense = Dense(128, activation='relu')(concat)
drop = Dropout(0.5)(dense)
output = Dense(1)(drop)

model = Model([user_input, book_input], output)
model.compile(loss='mse', optimizer='adam')
model.summary()

"""Model menggunakan dua input embedding untuk User dan Item (buku), masing-masing berdimensi 32. Hasil embedding diratakan (flatten), lalu digabungkan (concatenate) dan diproses oleh dua layer dense dengan dropout untuk regularisasi. Output model berupa prediksi rating tunggal. Total parameter yang dilatih sebanyak 585.473.

"""

# Melatih model neural collaborative filtering menggunakan data pelatihan selama 5 epoch dengan batch size 256.
history = model.fit(
    [X_train['user'], X_train['book']],
    y_train,
    validation_data=([X_test['user'], X_test['book']], y_test),
    epochs=5,
    batch_size=256
)

#Menghitung nilai RMSE
y_pred = model.predict([X_test['user'].values, X_test['book'].values])
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
print(f'RMSE: {rmse:.4f}')

"""Model Collaborative Filtering menghasilkan RMSE sebesar 1.6089, yang menunjukkan rata-rata kesalahan prediksi rating sekitar 1.6 poin dari nilai sebenarnya."""

plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.legend()
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Training vs Validation Loss')
plt.show()

"""Berdasarkan grafik di atas, **model menunjukkan penurunan nilai loss pada data latih dan validasi** seiring bertambahnya epoch, yang mengindikasikan bahwa **model berhasil belajar dan tidak mengalami overfitting**. Perbedaan nilai yang kecil antara train loss dan validation loss juga menunjukkan kestabilan performa model.

----
Selanjutnya adalah membuat fungsi rekomendasi buku berbasis collaborative filtering dengan input user_id. Fungsi ini akan mengidentifikasi buku-buku yang belum pernah diberi rating oleh pengguna tersebut, kemudian memprediksi rating potensial menggunakan model yang telah dilatih. Buku-buku dengan prediksi tertinggi akan direkomendasikan, dan hasilnya dikembalikan dalam bentuk daftar judul dan penulis. Pendekatan ini merekomendasikan buku berdasarkan pola interaksi pengguna lain yang mirip.
"""

def recommend_books(user_id, model, ratings_df, book_df, top_n=10):
    # Encode user_id asli ke index
    try:
        user_idx = user_enc.transform([user_id])[0]
    except:
        print("User tidak ditemukan.")
        return

    # Buku yang sudah diberi rating oleh user
    rated_books = ratings_df[ratings_df['user'] == user_idx]['book'].tolist()

    # Daftar semua buku
    all_books = np.arange(num_books)

    # Buku yang belum dirating
    unrated_books = np.setdiff1d(all_books, rated_books)

    # Buat prediksi
    user_input = np.full(len(unrated_books), user_idx)
    book_input = unrated_books

    predictions = model.predict([user_input, book_input], verbose=0)
    top_indices = predictions.flatten().argsort()[-top_n:][::-1]
    top_book_indices = unrated_books[top_indices]

    # Decode book index ke ISBN
    recommended_isbns = book_enc.inverse_transform(top_book_indices)

    # Ambil judul buku
    recommended_titles = book_df[book_df['ISBN'].isin(recommended_isbns)][['ISBN', 'Book-Title', 'Book-Author']]
    return recommended_titles.drop_duplicates('ISBN').head(top_n)

"""selanjutnya menyimpan model"""

#Save the trained model
model.save('collaborative_filtering_model.h5')

# You might also want to save the label encoders
with open('user_encoder.pkl', 'wb') as f:
    pickle.dump(user_enc, f)

with open('book_encoder.pkl', 'wb') as f:
    pickle.dump(book_enc, f)

"""Melakukan uji coba fungsi rekomendasi dengan memasukkan user_id tertentu. Sistem akan memprediksi rating dari buku-buku yang belum pernah dibaca oleh user tersebut, lalu menampilkan 5 buku teratas dengan prediksi rating tertinggi."""

#Contoh penggunaan
user_sample_id = 276747
recommend_books(user_sample_id, model, ratings, books, top_n=5)

"""berdasarkan hasil uji coba di atas, sistem  rekomendasi collaborative filltering menunjukkan lima buku teratas yang diprediksi paling relevan untuk user dengan ID 276747. Buku-buku yang direkomendasikan mencakup karya populer seperti “The Ultimate Hitchhiker’s Guide” oleh Douglas Adams, beberapa komik Calvin & Hobbes oleh Bill Watterson, serta novel klasik seperti “A Little Princess” dan “Pride and Prejudice”. Ini mengindikasikan bahwa sistem collaborative filtering mampu menangkap preferensi pengguna berdasarkan pola interaksi pengguna lain yang serupa.

# Kesimpulan

Percobaan sistem rekomendasi dilakukan dengan dua pendekatan, yaitu Content-Based Filtering dan Collaborative Filtering. Content-Based menggunakan TF-IDF dan cosine similarity untuk merekomendasikan buku yang mirip secara konten, namun hanya menghasilkan Precision\@5 sebesar 0.0093. Sementara itu, Collaborative Filtering yang memanfaatkan embedding dan jaringan saraf sederhana menunjukkan performa lebih baik dengan nilai RMSE sebesar 1.6089. Hal ini menunjukkan bahwa pendekatan Collaborative Filtering lebih efektif dalam memberikan rekomendasi yang relevan pada eksperimen ini.
"""